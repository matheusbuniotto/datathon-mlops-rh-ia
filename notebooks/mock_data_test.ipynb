{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8094f705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e75b552",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.read_parquet(\"../data/final/test_candidates_raw.parquet\").drop('target_rank', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a237a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import load\n",
    "from loguru import logger\n",
    "from app.utils.embedding_utils import explode_embeddings\n",
    "from app.stages.feature_engineering_stage import calculate_area_similarity\n",
    "from app.utils.strings_clean_utils import clean_area_atuacao\n",
    "\n",
    "def process_new_data(new_data, pipeline_path=\"../models/feature_pipeline.joblib\"):\n",
    "    \"\"\"\n",
    "    Processa novos dados usando o pipeline de features salvo.\n",
    "    \n",
    "    Args:\n",
    "        new_data (pd.DataFrame): DataFrame com os novos dados a serem processados\n",
    "        pipeline_path (str): Caminho para o pipeline salvo\n",
    "        \n",
    "    Returns:\n",
    "        scipy.sparse.csr_matrix: Dados processados prontos para o modelo\n",
    "    \"\"\"\n",
    "    # 1. Carrega o pipeline\n",
    "    logger.info(f\"Carregando pipeline de {pipeline_path}...\")\n",
    "    pipe = load(pipeline_path)\n",
    "    \n",
    "    # 2. Faz uma cópia para não modificar o DataFrame original\n",
    "    df_processed = new_data.copy()\n",
    "    \n",
    "    # 3. Aplica os mesmos pré-processamentos feitos em apply_feature_pipeline\n",
    "    logger.info(\"Aplicando transformações iniciais...\")\n",
    "    \n",
    "    # 3.1 Expande embeddings\n",
    "    df_processed = explode_embeddings(df_processed)\n",
    "    \n",
    "    # 3.2 Preenche valores faltantes\n",
    "    for col in df_processed.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df_processed[col]):\n",
    "            df_processed[col] = df_processed[col].fillna(-999)\n",
    "        else:\n",
    "            df_processed[col] = df_processed[col].fillna(\"Indefinido\")\n",
    "    \n",
    "    # 3.3 Limpa colunas de área de atuação\n",
    "    df_processed['candidato_area_atuacao'] = clean_area_atuacao(df_processed, 'candidato_area_atuacao')\n",
    "    df_processed['vaga_areas_atuacao_clean'] = clean_area_atuacao(df_processed, 'vaga_areas_atuacao')\n",
    "    \n",
    "    # 3.4 Calcula similaridade de área\n",
    "    df_processed = calculate_area_similarity(df_processed)\n",
    "    \n",
    "    # 4. Aplica as transformações do pipeline\n",
    "    logger.info(\"Aplicando transformações do pipeline...\")\n",
    "    X_processed = pipe.transform(df_processed)\n",
    "    \n",
    "    logger.success(\"Dados processados com sucesso!\")\n",
    "    return X_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fd3fa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-19 09:42:37.046\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_new_data\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mCarregando pipeline de ../models/feature_pipeline.joblib...\u001b[0m\n",
      "\u001b[32m2025-06-19 09:42:37.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_new_data\u001b[0m:\u001b[36m28\u001b[0m - \u001b[1mAplicando transformações iniciais...\u001b[0m\n",
      "/tmp/ipykernel_23628/73994828.py:42: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_processed['vaga_areas_atuacao_clean'] = clean_area_atuacao(df_processed, 'vaga_areas_atuacao')\n",
      "\u001b[32m2025-06-19 09:42:37.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_new_data\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mAplicando transformações do pipeline...\u001b[0m\n",
      "\u001b[32m2025-06-19 09:42:37.894\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_new_data\u001b[0m:\u001b[36m51\u001b[0m - \u001b[32m\u001b[1mDados processados com sucesso!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "X_novos = process_new_data(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f53939ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicoes = modeleao.predict(X_novos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "81f11cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9789077 , 1.28549868, 0.31257997, ..., 0.55091626, 1.0716712 ,\n",
       "       0.70600024], shape=(4273,))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7179100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-19 10:32:50.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpredict_rank_for_vaga\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mCarregando modelo e pipeline...\u001b[0m\n",
      "\u001b[32m2025-06-19 10:32:50.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpredict_rank_for_vaga\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mPré-processando os dados...\u001b[0m\n",
      "/tmp/ipykernel_23628/3109716850.py:54: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_vaga['vaga_areas_atuacao_clean'] = clean_area_atuacao(df_vaga, 'vaga_areas_atuacao')\n",
      "\u001b[32m2025-06-19 10:32:50.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpredict_rank_for_vaga\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mAplicando transformações do pipeline...\u001b[0m\n",
      "\u001b[32m2025-06-19 10:32:50.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpredict_rank_for_vaga\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mFazendo predições...\u001b[0m\n",
      "\u001b[32m2025-06-19 10:32:50.789\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpredict_rank_for_vaga\u001b[0m:\u001b[36m82\u001b[0m - \u001b[32m\u001b[1mRanking gerado com sucesso para vaga 4613\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vaga_id': np.int32(4613), 'candidatos': [{'nome_candidato': 'Apollo da Conceição', 'score': 0.9102, 'rank': 4}, {'nome_candidato': 'Dr. Daniel Rocha', 'score': 0.975, 'rank': 3}, {'nome_candidato': 'Ravi Lucca Ribeiro', 'score': 0.9789, 'rank': 2}, {'nome_candidato': 'Henrique Ferreira', 'score': 1.2317, 'rank': 1}]}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import load\n",
    "from loguru import logger\n",
    "from typing import List, Dict, Any\n",
    "from app.utils.embedding_utils import explode_embeddings\n",
    "from app.stages.feature_engineering_stage import calculate_area_similarity\n",
    "from app.utils.strings_clean_utils import clean_area_atuacao\n",
    "\n",
    "def predict_rank_for_vaga(df_candidates: pd.DataFrame, vaga_id: int, top_n: int = 5, \n",
    "                         model_path: str = \"../models/lgbm_ranker.pkl\",\n",
    "                         pipeline_path: str = \"../models/feature_pipeline.joblib\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Faz o predict e retorna o ranking de candidatos para uma vaga específica.\n",
    "    \n",
    "    Args:\n",
    "        df_candidates: DataFrame com os candidatos a serem ranqueados\n",
    "        vaga_id: ID da vaga para filtrar os candidatos\n",
    "        top_n: Número de candidatos a retornar no ranking\n",
    "        model_path: Caminho para o modelo treinado\n",
    "        pipeline_path: Caminho para o pipeline de features\n",
    "        \n",
    "    Returns:\n",
    "        Dicionário com o ID da vaga e a lista de candidatos ranqueados\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Filtra candidatos para a vaga específica\n",
    "        df_vaga = df_candidates[df_candidates[\"codigo_vaga\"] == vaga_id].copy()\n",
    "        \n",
    "        if df_vaga.empty:\n",
    "            logger.warning(f\"Nenhum candidato encontrado para a vaga {vaga_id}.\")\n",
    "            return {\"vaga_id\": vaga_id, \"candidatos\": []}\n",
    "        \n",
    "        # Carrega modelo e pipeline\n",
    "        logger.info(\"Carregando modelo e pipeline...\")\n",
    "        model = load(model_path)\n",
    "        pipe = load(pipeline_path)\n",
    "        \n",
    "        # Pré-processamento dos dados\n",
    "        logger.info(\"Pré-processando os dados...\")\n",
    "        \n",
    "        # 1. Expande embeddings\n",
    "        df_vaga = explode_embeddings(df_vaga)\n",
    "        \n",
    "        # 2. Preenche valores faltantes\n",
    "        for col in df_vaga.columns:\n",
    "            if pd.api.types.is_numeric_dtype(df_vaga[col]):\n",
    "                df_vaga[col] = df_vaga[col].fillna(-999)\n",
    "            else:\n",
    "                df_vaga[col] = df_vaga[col].fillna(\"Indefinido\")\n",
    "        \n",
    "        # 3. Limpa áreas de atuação\n",
    "        df_vaga['candidato_area_atuacao'] = clean_area_atuacao(df_vaga, 'candidato_area_atuacao')\n",
    "        df_vaga['vaga_areas_atuacao_clean'] = clean_area_atuacao(df_vaga, 'vaga_areas_atuacao')\n",
    "        \n",
    "        # 4. Calcula similaridade de área\n",
    "        df_vaga = calculate_area_similarity(df_vaga)\n",
    "\n",
    "\n",
    "        # 5. Aplica transformações do pipeline\n",
    "        logger.info(\"Aplicando transformações do pipeline...\")\n",
    "        X_processed = pipe.transform(df_vaga)\n",
    "        \n",
    "        \n",
    "        # 6. Faz predições\n",
    "        logger.info(\"Fazendo predições...\")\n",
    "        predictions = model.predict(X_processed)\n",
    "        \n",
    "        # 7. Cria DataFrame com resultados\n",
    "        results_df = pd.DataFrame({\n",
    "            'nome_candidato': df_vaga['nome_candidato'],\n",
    "            'score': np.round(predictions, 4),\n",
    "            'rank': predictions.argsort() + 1 ,\n",
    "\n",
    "        })\n",
    "        \n",
    "        # 8. Ordena por score e pega top N candidatos\n",
    "        top_candidates = (results_df.sort_values('rank', ascending=False)\n",
    "                        .head(top_n)\n",
    "                        .to_dict('records'))\n",
    "        \n",
    "        logger.success(f\"Ranking gerado com sucesso para vaga {vaga_id}\")\n",
    "        return {\n",
    "            \"vaga_id\": vaga_id,\n",
    "            \"candidatos\": top_candidates\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erro ao gerar ranking para vaga {vaga_id}: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "# Exemplo de uso:\n",
    "vaga_id_exemplo = df_merged['codigo_vaga'].iloc[0]\n",
    "resultado = predict_rank_for_vaga(df_merged, vaga_id_exemplo)\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc62da4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
