import numpy as np
from joblib import load
from scipy import sparse
from sklearn.metrics import ndcg_score, average_precision_score
from loguru import logger
import os
import json

DATA_PATH = "data/model_input/"
MODEL_PATH = "app/model/lgbm_ranker.pkl"


def load_inputs(dataset_type):
    """Carrega dados do conjunto especificado"""
    logger.info(f"[Eval] Carregando dados do conjunto: {dataset_type}")
    X = sparse.load_npz(os.path.join(DATA_PATH, f"X_{dataset_type}.npz"))
    y = np.load(os.path.join(DATA_PATH, f"y_{dataset_type}.npy"))
    group = np.load(os.path.join(DATA_PATH, f"group_{dataset_type}.npy"))
    logger.info(
        f"[Eval] Dados {dataset_type} carregados: {X.shape}, {y.shape}, {group.shape}"
    )
    return X, y, group


def evaluate_model(model, X, y, group, dataset_name, k=5, relevance_threshold=0):
    """Avalia modelo em um conjunto espec√≠fico"""
    logger.info(f"[Eval] Avaliando modelo no conjunto: {dataset_name}")

    y_pred = model.predict(X)

    scores_ndcg = []
    scores_map = []
    total_groups = 0
    valid_groups = 0

    offset = 0
    for size in group:
        y_true_group = y[offset : offset + size]
        y_pred_group = y_pred[offset : offset + size]
        total_groups += 1

        if len(np.unique(y_true_group)) > 1:
            valid_groups += 1

            # NDCG funciona com m√∫ltiplos n√≠veis de relev√¢ncia
            scores_ndcg.append(ndcg_score([y_true_group], [y_pred_group], k=k))

            # Para MAP, binarizamos os r√≥tulos (relevante vs n√£o relevante)
            y_true_binary = (y_true_group > relevance_threshold).astype(int)

            # S√≥ calcula MAP se houver pelo menos um item relevante
            if np.sum(y_true_binary) > 0:
                scores_map.append(average_precision_score(y_true_binary, y_pred_group))

        offset += size

    # Calcula m√©tricas
    ndcg_mean = np.mean(scores_ndcg) if scores_ndcg else 0.0
    map_mean = np.mean(scores_map) if scores_map else 0.0

    logger.success(f"[Eval {dataset_name.upper()}] NDCG@{k}: {ndcg_mean:.4f}")
    logger.success(f"[Eval {dataset_name.upper()}] MAP@{k}:  {map_mean:.4f}")
    logger.info(
        f"[Eval {dataset_name.upper()}] Grupos v√°lidos: {valid_groups}/{total_groups}"
    )

    return {
        "dataset": dataset_name,
        "ndcg": ndcg_mean,
        "map": map_mean,
        "total_groups": total_groups,
        "valid_groups": valid_groups,
        "k": k,
        "relevance_threshold": relevance_threshold,
    }


def comprehensive_evaluation(model_path, k=5, relevance_threshold=0):
    """
    Avalia√ß√£o completa seguindo melhores pr√°ticas:

    1. VAL: Para desenvolvimento e debugging (pode usar quantas vezes quiser)
    2. TEST: Para avalia√ß√£o final apenas (use UMA VEZ no final)
    """

    logger.info("=" * 60)
    logger.info("üéØ AVALIA√á√ÉO SEGUINDO MELHORES PR√ÅTICAS")
    logger.info("=" * 60)

    # Carrega modelo
    model = load(model_path)
    logger.success(f"[Eval] Modelo carregado de: {model_path}")

    results = {}

    # 1. AVALIA√á√ÉO NO CONJUNTO DE VALIDA√á√ÉO
    logger.info("\n" + "=" * 40)
    logger.info("üìä FASE 1: AVALIA√á√ÉO NO CONJUNTO DE VALIDA√á√ÉO")
    logger.info("üí° Use esta m√©trica para desenvolvimento e tuning")
    logger.info("=" * 40)

    try:
        X_val, y_val, group_val = load_inputs("val")
        results["validation"] = evaluate_model(
            model, X_val, y_val, group_val, "validation", k, relevance_threshold
        )
    except FileNotFoundError:
        logger.warning("[Eval] Conjunto de valida√ß√£o n√£o encontrado!")
        results["validation"] = None

    # 2. AVALIA√á√ÉO NO CONJUNTO DE TESTE
    logger.info("\n" + "=" * 40)
    logger.info("üîí FASE 2: AVALIA√á√ÉO FINAL NO CONJUNTO DE TESTE")
    logger.info("‚ö†Ô∏è  ATEN√á√ÉO: Use apenas UMA VEZ para avalia√ß√£o final!")
    logger.info("=" * 40)

    try:
        X_test, y_test, group_test = load_inputs("test")
        results["test"] = evaluate_model(
            model, X_test, y_test, group_test, "test", k, relevance_threshold
        )
    except FileNotFoundError:
        logger.error("[Eval] Conjunto de teste n√£o encontrado!")
        results["test"] = None

    # 3. RELAT√ìRIO FINAL
    logger.info("\n" + "=" * 50)
    logger.info("üìã RELAT√ìRIO FINAL DE AVALIA√á√ÉO")
    logger.info("=" * 50)

    if results["validation"]:
        val_ndcg = results["validation"]["ndcg"]
        logger.info("üîß VALIDA√á√ÉO (para desenvolvimento):")
        logger.info(f"   NDCG@{k}: {val_ndcg:.4f}")
        logger.info(f"   MAP@{k}:  {results['validation']['map']:.4f}")

    if results["test"]:
        test_ndcg = results["test"]["ndcg"]
        logger.info("üéØ TESTE (performance final):")
        logger.info(f"   NDCG@{k}: {test_ndcg:.4f}")
        logger.info(f"   MAP@{k}:  {results['test']['map']:.4f}")

        # An√°lise de overfitting
        if results["validation"]:
            diff = val_ndcg - test_ndcg
            if abs(diff) < 0.02:
                logger.success(f"‚úÖ Modelo bem generalizado (diff: {diff:+.4f})")
            elif diff > 0.02:
                logger.warning(f"‚ö†Ô∏è  Poss√≠vel overfitting (val-test: {diff:+.4f})")
            else:
                logger.info(f"ü§î Teste melhor que valida√ß√£o (diff: {diff:+.4f})")

    # Salva resultados
    results_path = "app/model/evaluation_results.json"
    with open(results_path, "w") as f:
        json.dump(results, f, indent=2)
    logger.info(f"üìÅ Resultados salvos em: {results_path}")

    return results


def development_evaluation(model_path, k=5, relevance_threshold=0):
    """
    Avalia√ß√£o APENAS no conjunto de valida√ß√£o
    Use esta fun√ß√£o durante o desenvolvimento/tuning
    """
    logger.info("üîß AVALIA√á√ÉO DE DESENVOLVIMENTO (apenas valida√ß√£o)")

    model = load(model_path)
    X_val, y_val, group_val = load_inputs("val")

    return evaluate_model(
        model, X_val, y_val, group_val, "validation", k, relevance_threshold
    )


def final_test_evaluation(model_path, k=5, relevance_threshold=0):
    """
    Avalia√ß√£o APENAS no conjunto de teste
    Use esta fun√ß√£o UMA VEZ para avalia√ß√£o final
    """
    logger.warning("üîí AVALIA√á√ÉO FINAL - USE APENAS UMA VEZ!")
    logger.warning("‚ö†Ô∏è  Esta √© sua avalia√ß√£o definitiva do modelo!")

    model = load(model_path)
    X_test, y_test, group_test = load_inputs("test")

    return evaluate_model(
        model, X_test, y_test, group_test, "test", k, relevance_threshold
    )


if __name__ == "__main__":
    import sys

    # Verifica argumentos da linha de comando
    evaluation_type = sys.argv[1] if len(sys.argv) > 1 else "comprehensive"

    if evaluation_type == "dev":
        # Para desenvolvimento - pode usar quantas vezes quiser
        logger.info("üîß Executando avalia√ß√£o de desenvolvimento...")
        result = development_evaluation(MODEL_PATH, k=5, relevance_threshold=0)

    elif evaluation_type == "final":
        # Para avalia√ß√£o final - use apenas uma vez!
        logger.warning("üîí Executando avalia√ß√£o final...")
        response = input("‚ö†Ô∏è  TEM CERTEZA? Esta √© a avalia√ß√£o final! (yes/no): ")
        if response.lower() == "yes":
            result = final_test_evaluation(MODEL_PATH, k=5, relevance_threshold=0.2)
        else:
            logger.info("Avalia√ß√£o cancelada.")
            exit()

    else:
        # Avalia√ß√£o completa (padr√£o)
        logger.info("üìä Executando avalia√ß√£o completa...")
        results = comprehensive_evaluation(MODEL_PATH, k=5, relevance_threshold=0.2)

    logger.success("‚úÖ Avalia√ß√£o conclu√≠da!")
